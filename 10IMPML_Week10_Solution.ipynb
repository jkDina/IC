{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10, Clustering\n",
    "\n",
    "**_Author: Jessica Cervi_**\n",
    "\n",
    "**Expected time = 2 hours**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment overview\n",
    "\n",
    "\n",
    "In Week 10, you leart about cluster analysis. \n",
    "Clustering can be defined as 'the process of organising objects into groups whose members are similar in some way'.\n",
    "Clustering is crucial because it determines the intrinsic grouping within unlabelled data. Clustering algorithms make some assumptions about data points to constitute their similarity. Thus, each hypothesis will construct different but equally valid clusters.\n",
    "\n",
    "For example, if you built a fruit classifier, it would say 'this is an orange, this is an apple', based on you showing it examples of apples and oranges.\n",
    "Clustering is the result of unsupervised learning, which means that you’ve seen lots of examples but don’t have labels. In this case, the clustering might return with 'fruits with soft skin and lots of dimples', 'fruits with shiny hard skin'  based merely on showing lots of fruit to the system, but not identifying the names of the different types of fruit. \n",
    "\n",
    "\n",
    "\n",
    "This assignment is designed to help you apply the machine learning algorithms you have learnt using packages in Python. Python concepts, instructions and a starter code are embedded within this Jupyter Notebook to guide you as you progress through the assignment. Remember to run the code of each code cell prior to submitting the assignment. Upon completing the assignment, we encourage you to compare your work against the solution file to perform a self-assessment.\n",
    "\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "\n",
    "- Define the concept of proximity for clustering methods\n",
    "- Outline the steps involved in hierarchical clustering\n",
    "- Understand why hierarchical clustering is myopic\n",
    "- Outline the steps involved in K-means clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index:\n",
    "\n",
    "#### Week 10:   Clustering\n",
    "\n",
    "\n",
    "- [Part 1](#part1)- Importing the data set and exploratory data analysis (EDA)\n",
    "- [Part 2](#part2)- Data preprocessing\n",
    "- [Part 3](#part3)- Identify the areas in the city that experience more criminal activity\n",
    "- [Part 4](#part4)- Clustering without normalisation\n",
    "- [Part 5](#part5)- Clustering with normalisation\n",
    "- [Part 6](#part6) - Hierarchical clustering\n",
    "- [Part 7](#part7) - A simple example of hierarchical clustering\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Clustering\n",
    "\n",
    "\n",
    "K-means clustering is a type of unsupervised learning, which is used when you have unlabelled data (i.e. data without defined categories or groups). The goal of this algorithm is to find groups in the data with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of the K groups based on the features that are provided. Data points are clustered based on the similarity in features. The results of the K-means clustering algorithm are as follows:\n",
    "\n",
    "- The centroids of the K clusters, which can be used to label new data\n",
    "\n",
    "- Labels for the training data (each data point is assigned to a single cluster)\n",
    "\n",
    "Rather than defining groups before looking at the data, clustering allows you to find and analyze the groups that have formed organically. The \"Choosing K\" section below describes how the number of groups can be determined.  \n",
    "\n",
    "Each centroid of a cluster is a collection of feature values which define the resulting groups. Examining the centroid feature weights can be used to qualitatively interpret the kind of group each cluster represents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising crimes in Chicago using clustering\n",
    "\n",
    "\n",
    "In this notebook, we will try and exploit information  regarding crimes in Chicago. We will perform the following steps:\n",
    "\n",
    "\n",
    "- 1. Read the `Crimes_2001_to_present_sample.csv` file in Python\n",
    "- 2. Drop the columns `X Coordinate`, `Y Coordinate`, `Updated On`, `Location`, `Beat`. Next, convert the column `Date` to datetime format and define two new columns, `date` and `time`.\n",
    "- 3. Define a dataframe, `sub_df` with the columns `Ward`, `IUCR` and  `District`. Impute all the missing values with the most frequent value in each column. Drop any non-numerical value in the column `IUCR`.\n",
    "- 4. Perform clustering without normalisation on the new dataframe. Compute the K-means for N=1,2,..,20 and their score to produce an elbow chart to predict the optimal number of clusters for this problem. Once you have identified this optimal number, recompute the K-mean.\n",
    "- 5. Perform clustering with normalisation on the new dataframe. Compute the K-means for N=1,2,..,20 and their score to produce an elbow chart to  predict the optimal number of clusters for this problem. Once you have identified this optimal number, recompute the K-mean.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part1'></a>\n",
    "\n",
    "### Part 1 -  Importing the data set and exploratory data analysis (EDA)\n",
    "\n",
    "The data set contains records from 2001 to present day, but with only 65k-66k records compared to the original dataset, which has around 6.6m records. If anyone is interested in analysing the original data set, it can be found [here](https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2).\n",
    "\n",
    "Nonetheless, 65k instances should be enough information to give us some good insights into the crime scene in Chicago. Let's get started by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will then use `pandas` to import the data set. Complete the code cell below by adding the name of the data set as a `str` to `.read_csv()`. Assign the dataframe to the variable `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Crimes_2001_to_present_sample.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before building any machine learning algorithms, we should explore the data.\n",
    "\n",
    "We begin by visualising the first ten rows of the dataframe `df` using the function `.head()`. By default, `.head()` displays the first five rows of a dataframe.\n",
    "\n",
    "Complete the code cell below by passing the desired number of rows as an `int` to the function `.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part2'></a>\n",
    "\n",
    "### Part 2 - Data preprocessing\n",
    "\n",
    "When building any machine learning algorithm, it is always useful, after analysing your data, to eliminate features that are redundant or that won't be useful in our analysis.\n",
    "\n",
    "Complete the code cell below to drop the columns `X Coordinate`, `Y Coordinate`, `Updated On`, `Location`, `Beat`. Remember to include the names of the columns in square brackets and that the axis parameter in `.drop()` controls whether the function acts on rows or columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['X Coordinate', 'Y Coordinate', 'Updated On', 'Location', 'Beat'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be useful for our analysis to know the date and the time when a certain crime happened. All this information is enclosed in the column `Date`. However, if you run the cell below, you may observe that the entries in `Date` are of type \"object\" instead of a date-specific object like `Timestamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.Date.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily,  `pandas` comes with a very powerful function, `datetime()`, that can convert date strings to the datetime format. Documentation about this function can be found [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html).\n",
    "\n",
    "Complete the code cell below by applying the function `datetime` on the column `Date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df.Date) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `datetime` is so powerful that it allows us to separate the date from the time.\n",
    "\n",
    "Run the code cell below to see how we create a new column `date` containing just the date of the crime. This cell uses list comprehension to convert each date and time in `Date` to a date. The resulting list is reassigned to the column `date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = [d.date() for d in df['Date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the syntax used in the code cell above, complete the cell below to create a columns `time` containing the time at which each crime happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'] = [d.time() for d in df['Date']]\n",
    "\n",
    "#df['time'] ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the new dataframe. Notice the two new columns we have just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part3'></a>\n",
    "\n",
    "### Part 3 -  Identify the areas in the city that experience more criminal activity\n",
    "\n",
    "To identify which portions of the city experience criminal attacks of which type, it may be useful to  cluster the data according to the `District`, `Ward` and `Primary Type`(as per IUCR [Illinois Uniform Crime Reporting] code). IUCR codes are four digit codes that law enforcement agencies use to classify criminal incidents when taking individual reports, you can find more information [here](https://data.cityofchicago.org/Public-Safety/Chicago-Police-Department-Illinois-Uniform-Crime-R/c7ck-438e/data)).\n",
    "\n",
    "Because we are only interested in the features `District`, `Ward` and `Primary Type`, it may be convenient to select only those features from the original dataframe.\n",
    "\n",
    "Complete the cell below by extracting  `District`, `Ward` and `Primary Type` from `df` and assign the new dataframe to `sub_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = df[['Ward', 'IUCR', 'District']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise our new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have some missing values (NaNs). The general technique to fill missing values in a dataframe is call **imputation**.\n",
    "\n",
    "A very popular way to impute values is by filling missing values with the most frequent value from one column. This can be done by using the function `.apply()` on our dataframe together with the appropriate lambda function.\n",
    "\n",
    "Complete the lambda function that needs to be passed to `.apply()` in the code cell below.\n",
    "\n",
    "**HINT:** Use a combination of the functions `fillna()`, `value_counts()` and `index()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = sub_df.apply(lambda x:x.fillna(x.value_counts().index[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before moving on to clustering we will extract the numeric values in the columns`IUCR` and visualize our new dataframe.\n",
    "\n",
    "Run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = sub_df.assign(IUCR = sub_df.IUCR.str.extract('(\\d+)', expand=True).astype(int))\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part4'></a>\n",
    "\n",
    "### Part 4 - Clustering without normalisation\n",
    "\n",
    "Before getting into clustering, we should point out a few things:\n",
    "\n",
    "To find the **optimal number of clusters**, we will go with the elbow rule, which states that on the curve of score vs the number of clusters, the optimal point is where the first bend (or *elbow*) occurs, because after that the the score eventually decreases to zero and each point starts behaving as its own cluster. \n",
    "\n",
    "\n",
    "However, with K-means, we will have to normalise the data first as, without it, K-means will simply cluster the data based on the Euclidean distances of the IUCR code, as it has a larger range of values than the District or Ward codes. We will begin by finding the optimal number of clusters *without normalisation*.\n",
    "\n",
    "\n",
    "Complete the code cell below by importing `Kmeans` from `sklearn.cluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to reproduce the \"elbow chart\", we need to compute the score for each number of clusters.\n",
    "\n",
    "First, we need a list of models defined for a varying number of clusters. Then, we can fit each model on the data to get a score for the model and determine the model with the best number of clusters.\n",
    "\n",
    "Complete the for loop in the code cell below to compute the K-means for the given range of values. This for loop returns a list of `Kmeans` model objects, each of which has a different number of clusters ranging from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = range(1, 10)\n",
    "kmeans = []\n",
    "for i in N:\n",
    "    kmeans.append(KMeans(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's compute the score. Run the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "for i in range(len(kmeans)):\n",
    "    score.append(kmeans[i].fit(sub_df).score(sub_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot our elbow curve. Run the code below to plot the elbow curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot(N,score)\n",
    "pl.xlabel('Number of Clusters')\n",
    "pl.ylabel('Score')\n",
    "pl.title('Elbow Curve')\n",
    "pl.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the optimal number of clusters seem to be?\n",
    "\n",
    "**DOUBLE CLICK ON THIS CELL TO TYPE YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without normalising the data, the best number of clusters is around 4, so let's try that out!\n",
    "\n",
    "Complete the code cell below by following these steps:\n",
    "- Use `KMeans()` to compute the optimal K-mean `km` by choosing the optimal number of clusters.\n",
    "- Use `.fit()` to compute K-means clustering.\n",
    "- Use the `km` attribute `predict()` on `sub_df` to predict the closest cluster each sample belongs to.\n",
    "\n",
    "You can find the documentation about these functions [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=4)\n",
    "km.fit(sub_df)\n",
    "y = km.predict(sub_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Run the code cell below to define the clusters for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['Cluster'] = y\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to visualise the clusters obtained without normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "x = np.array(sub_df['Ward'])\n",
    "y = np.array(sub_df['IUCR'])\n",
    "z = np.array(sub_df['District'])\n",
    "\n",
    "ax.set_xlabel('Ward')\n",
    "ax.set_ylabel('IUCR')\n",
    "ax.set_zlabel('District')\n",
    "\n",
    "ax.scatter(x,y,z, marker=\"o\", c = sub_df[\"Cluster\"], s=60, cmap=\"jet\")\n",
    "ax.view_init(azim=0)\n",
    "#print(ax.azim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part5'></a>\n",
    "\n",
    "### Part 5 - Clustering with normalisation\n",
    "\n",
    "As expected, K-means without normalisation simply clusters the data based on the Euclidean distances of the IUCR codes. We can fix this by normalising the data.\n",
    "\n",
    "In the code cell below, we have normalised the column `IUCR` using the formula:\n",
    "\n",
    "$$y_\\text{norm} = \\frac{y - \\min(y)}{\\max(y) - \\min(y)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['IUCR'] = (sub_df['IUCR'] - sub_df['IUCR'].min())/(sub_df['IUCR'].max()-sub_df['IUCR'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code cell below to normalise the columns `Ward` and `District` following the example given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['Ward'] = (sub_df['Ward'] - sub_df['Ward'].min())/(sub_df['Ward'].max()-sub_df['Ward'].min())\n",
    "sub_df['District'] = (sub_df['District'] - sub_df['District'].min())/(sub_df['District'].max()-sub_df['District'].min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following what we did in Part 4, compute the updated K-means and scores for the normalised data.\n",
    "\n",
    "Complete the code cell below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = range(1, 20)\n",
    "kmeans_norm = []\n",
    "for i in N:\n",
    "    kmeans_norm.append(KMeans(i))\n",
    "\n",
    "    \n",
    "score_norm = []\n",
    "for i in range(len(kmeans_norm)):\n",
    "    score_norm.append(kmeans_norm[i].fit(sub_df).score(sub_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot our new elbow curve. Run the code below to plot the elbow curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot(N,score_norm)\n",
    "pl.xlabel('Number of Clusters - Normalized')\n",
    "pl.ylabel('Score')\n",
    "pl.title('Elbow Curve')\n",
    "pl.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the optimal number of clusters seem to be now?\n",
    "\n",
    "**DOUBLE CLICK ON THIS CELL TO TYPE YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With normalised data, the optimal number of clusters seems to be two now!\n",
    "\n",
    "Following what we did in Part 4, run `KMeans` with the updated number of clusters. Don't forget to include all the steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_norm = KMeans(n_clusters=2)\n",
    "km_norm.fit(sub_df)\n",
    "y_norm = km_norm.predict(sub_df)\n",
    "labels_norm = km_norm.labels_\n",
    "sub_df['Clusters_norm'] = y_norm\n",
    "sub_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the updated clusters look like.\n",
    "\n",
    "![](cluster_norm.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What differences can you infer from this updated graph? Is it a good or  bad thing?\n",
    "\n",
    "**DOUBLE CLICK ON THIS CELL TO TYPE YOUR ANSWER**\n",
    "\n",
    "(Correct answer (not to be shown to the students):The clustering doesn't seem to be based solely on the euclidean distances of IUCR codes now, which is good! )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part6'></a>\n",
    "\n",
    "### Part 6 - Hierarchical clustering\n",
    "    \n",
    "During this week, you have also learnt about another type of clustering technique called **hierarchical clustering**.\n",
    "\n",
    "\n",
    "Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other, and the objects within each cluster are broadly similar to each other.\n",
    "\n",
    "#### How hierarchical clustering works\n",
    "Hierarchical clustering starts by treating each observation as a separate cluster. Then, it repeatedly executes the following two steps: (1) identify the two clusters that are closest together and (2) merge the two most similar clusters. This iterative process continues until all the clusters are merged together. \n",
    "\n",
    "\n",
    "![](cluster_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main output of hierarchical clustering is a **dendrogram**, which shows the hierarchical relationship between the clusters:\n",
    "\n",
    "\n",
    "![](cluster_2.png)\n",
    "\n",
    "\n",
    "#### Measures of distance (similarity)\n",
    "In the example above, the distance between two clusters has been computed based on the length of the straight line drawn from one cluster to the other. This is commonly referred to as the Euclidean distance. Many other distance metrics have been developed.\n",
    "\n",
    "The choice of distance metric should be made based on theoretical concerns from the domain of study. That is, a distance metric needs to define similarity in a way that is sensible for the field of study. For example, if clustering crime sites in a city (like in the worked example above), the city block distance may be appropriate. Or, better yet, the time taken to travel between each location. Where there is no theoretical justification for an alternative, the Euclidean should generally be preferred, as it is usually the appropriate measure of distance in the physical world.\n",
    "\n",
    "\n",
    "#### Linkage criteria\n",
    "After selecting a distance metric, it is necessary to determine from where distance is computed. For example, it can be computed between the two most similar parts of a cluster (single-linkage), the two least similar bits of a cluster (complete-linkage), the centre of the clusters (mean or average-linkage), or some other criterion. Many linkage criteria have been developed.\n",
    "\n",
    "As with distance metrics, the choice of linkage criteria should be made based on theoretical considerations from the domain of application. A key theoretical issue is what causes variation. For example, in archaeology, we expect variation to occur through innovation and natural resources, so working out if two groups of artifacts are similar may make sense based on identifying the most similar members of the cluster.\n",
    "\n",
    "\n",
    "### Steps to perform hierarchical clustering\n",
    "\n",
    "- Step 1: First, we assign all the points to an individual cluster.\n",
    "- Step 2: Next, we will look at the smallest distance in the proximity matrix and merge the points with the smallest distance.\n",
    "- Step 3: We will repeat Step 2 until only a single cluster is left.\n",
    "- Step 4: How should we choose the number of clusters in hierarchical clustering?\n",
    "\n",
    "To get the number of clusters for hierarchical clustering, we make use of an awesome concept called a dendrogram.\n",
    "Whenever two clusters are merged, we will join them in this dendrogram and the height of the join will be the distance between these points. Therefore, the longer the distance of the vertical lines in the dendrogram, the longer the distance between those clusters.\n",
    "\n",
    "Finally, we can set a threshold distance and draw a horizontal line across the dendrogram. The number of clusters will be the number of vertical lines which are being intersected by the line drawn using the threshold.\n",
    "\n",
    "\n",
    "![](cluster_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "<a id='part7'></a>\n",
    "\n",
    "### Part 7 - A simple example of hierarchical clustering\n",
    "\n",
    "After seeing what the basic steps of hierarchical clustering are, it's now time to walk through a simple example to test our understanding.\n",
    "\n",
    "We will be working on a wholesale customer segmentation problem. You can download the data set using [this](https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv) link. The data is hosted on the UCI Machine Learning repository. \n",
    "\n",
    "The aim of this problem is to segment the clients of a wholesale distributor based on their annual spending on diverse product categories such as milk, grocery, region, etc.\n",
    "\n",
    "As usual, we begin by importing the data set, Complete the code cell below by adding the name of the data set, \"Wholesale customers data.csv\" a `str` to `.read_csv()`. Assign the dataframe to the variable `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Wholesale customers data.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to visualise the first 5 rows of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple product categories – Fresh, Milk, Grocery, etc. The values represent the number of units purchased by each client for each product. \n",
    "\n",
    "Our aim is to make clusters from this data that can segment similar clients together. Of course, hierarchical clustering seems to be the right strategy to solve this problem.\n",
    "\n",
    "\n",
    "\n",
    "In Part 5 of this assignment, we saw how important it is to normalise the data so that the scale of each variable is the same. \n",
    "\n",
    "Why is this important?\n",
    "\n",
    "**DOUBLE CLICK ON THIS CELL TO TYPE YOUR ANSWER**\n",
    "\n",
    "(Correct answer (not to be shown to the students) If the scale of the variables is not the same, the model might become biased towards the variables with a higher magnitude like Fresh or Milk (refer to the above table))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to import the function `normalize()` from `scikit-learn` that we will use to normalise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code cell below by normalising the dataframe `data`. Assign the new data to the object `data_scaled`.\n",
    "\n",
    "Next, use the `pandas` function `DataFrame()` to convert `data_scaled` to a dataframe `data_scaled`.\n",
    "For convenience, set the parameter `columns = data.columns` to keep the original column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = normalize(data)\n",
    "data_scaled = pd.DataFrame(data_scaled, columns=data.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to visualise the normalised dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next,  let’s first draw the dendrogram to help us decide the number of clusters for this particular problem.\n",
    "\n",
    "Run the code cell below to create the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "plt.figure(figsize=(10, 7))  \n",
    "plt.title(\"Dendrogram\")  \n",
    "dend = shc.dendrogram(shc.linkage(data_scaled, method='ward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x-axis contains the samples and y-axis represents the distance between these samples. The vertical line with maximum distance is the blue line and hence, we can decide on a threshold of 6 and cut the dendrogram.\n",
    "\n",
    "Run the code cell below to visualise the dendrogram with the threshold line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))  \n",
    "plt.title(\"Dendrogram\")  \n",
    "dend = shc.dendrogram(shc.linkage(data_scaled, method='ward'))\n",
    "plt.axhline(y=6, color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many clusters do we have at this point?\n",
    "\n",
    "**DOUBLE CLICK ON THIS CELL TO TYPE YOUR ANSWER**\n",
    "\n",
    "(Correct answer (not to be shown to the students) We have two clusters as this line cuts the dendrogram at two points.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `AgglomerativeClustering()` from `scikit-learn` to apply hierarchical clustering for two clusters. The documentation about this function can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code cell below by setting the following parameters for `AgglomerativeClustering`\n",
    "    \n",
    "- `n_clusters = 2`\n",
    "- `affinity='euclidean`\n",
    "- `linkage='ward`\n",
    "\n",
    "Finally, use the `fit_predict()` of the scaled dataframe to apply the clustering to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')  \n",
    "cluster.fit_predict(data_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why can we only see the values of 0s and 1s in the output?\n",
    "\n",
    "\n",
    "**DOUBLE CLICK ON THIS CELL TO TYPE YOUR ANSWER**\n",
    "\n",
    "(Correct answer (not to be shown to the students) Because we have defined 2 clusters. 0 represents the points that belong to the first cluster and 1 represents points in the second cluster. )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let’s now visualise the two clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))  \n",
    "plt.scatter(data_scaled['Milk'], data_scaled['Grocery'], c=cluster.labels_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! As expected, we can clearly visualise the two clusters.\n",
    "\n",
    "CONGRATULATIONS ON COMPLETING THR WEEK 10 ASSIGNMENT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
